{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "from heatmap import heatmaps_from_keypoints\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "from torch.autograd.variable import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAIN_FOLDER = \"/Volumes/TOSHIBA EXT/data/\"\n",
    "IMAGES_FOLDER = os.path.join(MAIN_FOLDER, \"train2017\")\n",
    "IMAGES_FOLDER_TEST = os.path.join(MAIN_FOLDER, \"val2017\")\n",
    "ANNOTATION_FILE = os.path.join(MAIN_FOLDER, \"annotations/person_keypoints_train2017.json\")\n",
    "ANNOTATION_FILE_TEST = os.path.join(MAIN_FOLDER, \"annotations/person_keypoints_val2017.json\")\n",
    "CHECKPOINTS_FOLDER = \"./cktp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_heatmap(shape, keypoint_coordinates, std = 1.5):\n",
    "    \"\"\"\n",
    "        Computes a square gaussian kernel\n",
    "\n",
    "        :param shape: Shape of the output heatmap\n",
    "        :param keypoint_coordinates: Location of the keypoint\n",
    "        :param std: Standard deviation\n",
    "\n",
    "        :return: Heatmap of shape (1,shape,shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the coordinates\n",
    "    x = keypoint_coordinates[0]\n",
    "    y = keypoint_coordinates[1]\n",
    "    \n",
    "    a = np.arange(0, shape, 1, float)\n",
    "    b = a[:,np.newaxis]\n",
    "\n",
    "    # Generate the heatmap\n",
    "    heatmap_raw = np.exp(-(((a-x)**2)/(2*std**2) + ((b-y)**2)/(2*std**2)))\n",
    "    \n",
    "    # Normalize\n",
    "    heatmap_max = np.amax(heatmap_raw)\n",
    "    heatmap_normalized = heatmap_raw/heatmap_max\n",
    "    \n",
    "    # Get it in the accurate format\n",
    "    heatmap = np.expand_dims(heatmap_raw, axis=0)\n",
    "    return heatmap\n",
    "\n",
    "def gaussian_heatmaps(xs, ys, vs, shape=32, image_height=512, image_width=640, std=1.):\n",
    "    \"\"\"\n",
    "        Computes heatmaps from the keypoints\n",
    "        :param xs: Array of x coordinates for the keypoints\n",
    "        :param ys: Array of y coordinates for the keypoints\n",
    "        :param shape: shape of the heatmaps\n",
    "        :param image_height: Height of the images the keypoints are for\n",
    "        :param image_width: Width of the images the keypoints are for\n",
    "        :param std: Standard deviation of the gaussion function used\n",
    "        \n",
    "        :return: Heatmaps as numpy arrays of shape (shape, shape, n_keypoints)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rescale keypoints coordinates to the heatmaps scale\n",
    "    # ys\n",
    "    height_scale = shape/image_height\n",
    "    ys = ys*height_scale\n",
    "    # xs\n",
    "    width_scale = shape/image_width\n",
    "    xs = xs*width_scale\n",
    "    \n",
    "    \n",
    "    # Render a heatmap for each joint\n",
    "    heatmaps = gaussian_heatmap(shape, (xs[0],ys[0]))\n",
    "    for i, v in enumerate(vs):\n",
    "        if i!=0:\n",
    "            # If the joint is visible, generate a heatmaps\n",
    "            if v!=0:\n",
    "                new_heatmap = gaussian_heatmap(shape, (xs[i],ys[i]))\n",
    "            # Otherwise the heatmaps is composed of zeros\n",
    "            else:\n",
    "                new_heatmap = np.zeros((1, shape, shape))\n",
    "            heatmaps = np.append(heatmaps, new_heatmap, axis=0)\n",
    "\n",
    "    return heatmaps\n",
    "\n",
    "def keypoints_from_heatmap(heatmap):\n",
    "    \"\"\"Get the coordinates of the max value heatmap - it is the keypoint\"\"\"\n",
    "    max_heatmap = np.amax(heatmap)\n",
    "    keypoints = np.where(heatmap == max_heatmap)\n",
    "    if len(keypoints) == 2:\n",
    "        return keypoints[1][0], keypoints[0][0], max_heatmap\n",
    "        \n",
    "    elif len(keypoints) == 3:\n",
    "        return keypoints[2][0], keypoints[1][0], max_heatmap\n",
    "\n",
    "def keypoints_from_heatmaps(heatmaps, shape=32, image_height=512, image_width=640):\n",
    "    \"\"\"Get the coordinates of the keypoints from the 17 heatmaps\"\"\"\n",
    "    keypoints = []\n",
    "    for i, heatmap in enumerate(heatmaps):\n",
    "        x, y, max_heatmap = keypoints_from_heatmap(heatmap)\n",
    "        if max_heatmap == 0:\n",
    "            keypoints += [0,0,0]\n",
    "        else:\n",
    "            x = x*image_width/shape\n",
    "            y = y*image_height/shape\n",
    "            keypoints += [x,y,2]\n",
    "    return keypoints\n",
    "\n",
    "def get_xs_ys_vs(keypoints):\n",
    "    \"\"\" Splits MSCOCO keypoints notations from [x0, y0, v0, ...] to [x0, ...], [y0, ...] and [v0, ...] \"\"\"\n",
    "    keypoints_array = np.asarray(keypoints)\n",
    "    xs = np.take(keypoints_array, [3*i for i in range(17)])\n",
    "    ys = np.take(keypoints_array, [3*i+1 for i in range(17)])\n",
    "    vs = np.take(keypoints_array, [3*i+2 for i in range(17)])\n",
    "    return xs, ys, vs\n",
    "\n",
    "def heatmaps_from_keypoints(keypoints):\n",
    "    xs, ys, vs = get_xs_ys_vs(keypoints)\n",
    "    heatmaps = gaussian_heatmaps(xs, ys, vs)\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSCOCO(data.Dataset):\n",
    "    \"\"\" Represents a MSCOCO Keypoints dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, images_folder, annotations_json, train=False, evalu=False, input_type=0):\n",
    "        \"\"\" Instantiate a MSCOCO dataset \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.images_folder = images_folder\n",
    "        #Input type indicates if the input is the original image or a combination of original image with filtered image\n",
    "        #O : original image\n",
    "        #1 : original image + skin filtered \n",
    "        #2 : original image + edge filter \n",
    "        #3 : original image + clustering filter \n",
    "        #4 : orignal image + skin filter + edge filter\n",
    "        #5 : orignal image + skin filter + clustering filter\n",
    "        self.input_type = input_type\n",
    "        \n",
    "        # Load the annotations\n",
    "        self.annotations = COCO(annotations_json)\n",
    "        imgs_id = self.annotations.getImgIds()\n",
    "        if train:\n",
    "            self.img_ids = imgs_id[:int(len(imgs_id)*2/3)]\n",
    "        \n",
    "        elif evalu:\n",
    "            self.img_ids = imgs_id[int(len(imgs_id)*2/3)+1:]\n",
    "        \n",
    "        else:\n",
    "            self.img_ids = imgs_id        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns the index-th image with keypoints annotations, both as tensors \"\"\"\n",
    "        \n",
    "        try:\n",
    "            #L is the list of the input's path for a single image\n",
    "            L = []\n",
    "            input_imgs = []\n",
    "\n",
    "            # Get the image informations\n",
    "            img_id = self.img_ids[index]\n",
    "            img = self.annotations.loadImgs(img_id)[0]\n",
    "            \n",
    "            # Load the image from the file\n",
    "            img_path = os.path.join(self.images_folder, img['file_name'])\n",
    "            L.append(img_path)\n",
    "            \n",
    "            #Need to adapt it depending on the path of the filtered image\n",
    "            if self.input_type == 1 or self.input_type == 4 or self.input_type == 5:\n",
    "                L.append(img_path) #Need to change with skin filtered image\n",
    "            if self.input_type == 2 or self.input_type == 4:\n",
    "                L.append(img_path) #Need to change with edge filtered image\n",
    "            if self.input_type == 3 or self.input_type == 5:\n",
    "                L.append(img_path) #Need to change with clustering filtered image\n",
    "            \n",
    "            for image in L:\n",
    "                img_array = load_image(image)\n",
    "                img_array = MSCOCO.transformGreyImage(img_array)\n",
    "                img_tensor = torch.from_numpy(img_array)\n",
    "                img_tensor = img_tensor.float() # Pytorch needs a float tensor\n",
    "                input_imgs.append(img_tensor)\n",
    "                \n",
    "            # Get the keypoints\n",
    "            annIds = self.annotations.getAnnIds(imgIds=img['id'])\n",
    "            anns = self.annotations.loadAnns(annIds)\n",
    "            # Some images do not contain any coco object, so anns = []\n",
    "            if len(anns)>0:\n",
    "                keypoints = anns[0]['keypoints'] # anns is a list with only one element\n",
    "            else:\n",
    "                # keypoints are not visible so \n",
    "                keypoints = [0 for i in range(3*17)]\n",
    "                \n",
    "            # Check to avoid errors\n",
    "            if len(keypoints)!=3*17:\n",
    "                print('Warning: Keypoints list for image {} has length {} instead of 17'.format(img_id, len(keypoints)))\n",
    "        \n",
    "            # Generate the heatmaps\n",
    "            heatmaps_array = heatmaps_from_keypoints(keypoints)\n",
    "            \n",
    "            #img_tensor_input = torch.cat((img_tensor,img_tensor_filtered),0)\n",
    "            keypoints_tensor = torch.from_numpy(heatmaps_array).float() # Pytorch needs a float tensor\n",
    "            img_tensor = torch.cat(input_imgs,0)\n",
    "            \n",
    "            return img_tensor, keypoints_tensor\n",
    "\n",
    "        except:\n",
    "            #L is the list of the input's path for a single image\n",
    "            L = []\n",
    "            input_imgs = []\n",
    "\n",
    "            # Get the image informations\n",
    "            img_id = 391895\n",
    "            img = self.annotations.loadImgs(img_id)[0]\n",
    "            \n",
    "            # Load the image from the file\n",
    "            img_path = os.path.join(self.images_folder, img['file_name'])\n",
    "            L.append(img_path)\n",
    "            \n",
    "            #Need to adapt it depending on the path of the filtered image\n",
    "            if self.input_type == 1 or self.input_type == 4 or self.input_type == 5:\n",
    "                L.append(img_path) #Need to change with skin filtered image\n",
    "            if self.input_type == 2 or self.input_type == 4:\n",
    "                L.append(img_path) #Need to change with edge filtered image\n",
    "            if self.input_type == 3 or self.input_type == 5:\n",
    "                L.append(img_path) #Need to change with clustering filtered image\n",
    "            \n",
    "            for image in L:\n",
    "                img_array = load_image(image)\n",
    "                img_array = MSCOCO.transformGreyImage(img_array)\n",
    "                img_tensor = torch.from_numpy(img_array)\n",
    "                img_tensor = img_tensor.float() # Pytorch needs a float tensor\n",
    "                input_imgs.append(img_tensor)\n",
    "                \n",
    "            # Get the keypoints\n",
    "            annIds = self.annotations.getAnnIds(imgIds=img['id'])\n",
    "            anns = self.annotations.loadAnns(annIds)\n",
    "            # Some images do not contain any coco object, so anns = []\n",
    "            if len(anns)>0:\n",
    "                keypoints = anns[0]['keypoints'] # anns is a list with only one element\n",
    "            else:\n",
    "                # keypoints are not visible so \n",
    "                keypoints = [0 for i in range(3*17)]\n",
    "                \n",
    "            # Check to avoid errors\n",
    "            if len(keypoints)!=3*17:\n",
    "                print('Warning: Keypoints list for image {} has length {} instead of 17'.format(img_id, len(keypoints)))\n",
    "        \n",
    "            # Generate the heatmaps\n",
    "            heatmaps_array = heatmaps_from_keypoints(keypoints)\n",
    "            \n",
    "            #img_tensor_input = torch.cat((img_tensor,img_tensor_filtered),0)\n",
    "            keypoints_tensor = torch.from_numpy(heatmaps_array).float() # Pytorch needs a float tensor\n",
    "            img_tensor = torch.cat(input_imgs,0)\n",
    "            \n",
    "            return img_tensor, keypoints_tensor \n",
    "\n",
    "    @staticmethod\n",
    "    def transformGreyImage(img_array):\n",
    "        # Black and white images\n",
    "        if len(img_array.shape)==2:\n",
    "            # Add a channel axis\n",
    "            img_array = np.expand_dims(img_array, axis=2)\n",
    "            # Fill all the axes with the black&white image\n",
    "            img_array = np.concatenate((img_array, img_array, img_array), axis=2)\n",
    "        img_array = np.transpose(img_array, (2,1,0))\n",
    "        return img_array\n",
    "\n",
    "\n",
    "# Homemade image loader\n",
    "def load_image(image_path):\n",
    "    image = imread(image_path)\n",
    "    image = resize(image, (256, 256))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, training=True, padding=1, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                            out_channels,\n",
    "                            kernel_size,\n",
    "                            padding=padding,\n",
    "                            stride=stride)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.training = training\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))\n",
    "        if self.training:\n",
    "            x = self.batch_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_type=0):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        #1 image\n",
    "        if input_type == 0:\n",
    "            input_size = 3\n",
    "        \n",
    "        #2 images\n",
    "        elif input_type == 1 or input_type == 2 or input_type == 3:\n",
    "            input_size = 6\n",
    "        \n",
    "        #3 images\n",
    "        elif input_type == 4 or input_type == 5:\n",
    "            input_size = 9\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "                ConvRelu(input_size, 64, 3),\n",
    "                ConvRelu(64, 64, 3),\n",
    "                self.pool,\n",
    "                ConvRelu(64, 128, 3),\n",
    "                #ConvRelu(128, 128, 3),\n",
    "                self.pool,\n",
    "                ConvRelu(128, 128, 3),\n",
    "                #ConvRelu(128, 128, 3),\n",
    "                self.pool,\n",
    "                ConvRelu(128, 512, 3),\n",
    "                #ConvRelu(512, 512, 3),\n",
    "                )\n",
    "        \n",
    "        self.features_to_heatmaps = nn.Conv2d(512, 17, 1) # 17 kind of joints, 17 heatmaps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extraction(x)\n",
    "        heatmaps = self.features_to_heatmaps(x)\n",
    "        return heatmaps\n",
    "\n",
    "def plotKeypointsOverOutputModel(index,dataset,model,img_folder):\n",
    "    \"\"\"Forward a img to the model and display the output keypoints over the image.\n",
    "       It enables us to see the loss evolution over the model visually over the image\n",
    "       index is the index of the img in the dataset argument\"\"\"\n",
    "    # Get an image\n",
    "    imgId = dataset.img_ids[index]\n",
    "    img, keypoints = dataset[index]\n",
    "\n",
    "    # Transform into a pytorch model input and Forward pass \n",
    "    y = model(Variable(img.unsqueeze(0)))\n",
    "\n",
    "    #Get the coordinates of the keypoints\n",
    "    keypoints = keypoints_from_heatmaps(y[0].data.numpy())\n",
    "\n",
    "    # Plot the image\n",
    "    img_anno = dataset.annotations.loadImgs(imgId)[0]\n",
    "    img_path = os.path.join(img_folder, img_anno['file_name'])\n",
    "    img_array = load_image(img_path)\n",
    "    img_array_resized = resize(img_array, (512, 640))\n",
    "    plt.figure()\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(img_array_resized)\n",
    "    xs,ys,vs = get_xs_ys_vs(keypoints)\n",
    "    plt.plot(xs,ys,'ro',color='c')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conf_training(resuming=False, input_type=0, *args):\n",
    "    \"\"\"Function that initiates the configuration of the model depending if a last model\n",
    "       is loaded or if it's the beginning of a new model\"\"\"\n",
    "    \n",
    "    #Data\n",
    "    trainset = MSCOCO(IMAGES_FOLDER, ANNOTATION_FILE, train=True, input_type=input_type)\n",
    "    evalset = MSCOCO(IMAGES_FOLDER, ANNOTATION_FILE, evalu=True, input_type=input_type)\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.MSELoss()\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Number of epochs\n",
    "    epochs = 10\n",
    "\n",
    "    # Batch sizes\n",
    "    batch_size_train = 1\n",
    "    batch_size_val = 1\n",
    "    \n",
    "    if not resuming:\n",
    "        # Model\n",
    "        net = Model(input_type=input_type)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "        \n",
    "        #First epoch\n",
    "        current_epoch = -1\n",
    "    \n",
    "    else:\n",
    "        #Load the last saved model with its configurations\n",
    "        checkpoint = torch.load(os.path.join(MAIN_FOLDER,\"model_\"+args[0]))\n",
    "        \n",
    "        #Model\n",
    "        net = Model(input_type=input_type)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        #Current_epoch\n",
    "        current_epoch = checkpoint['epoch']\n",
    "        \n",
    "        #Optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "    \n",
    "    #Data loaders\n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                         batch_size=batch_size_train,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=4\n",
    "                                        )\n",
    "\n",
    "    evaloader = torch.utils.data.DataLoader(evalset,\n",
    "                                         batch_size=batch_size_val,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=4\n",
    "                                        )\n",
    "    \n",
    "    evalset_length = len(evalset)\n",
    "    \n",
    "    return epochs, trainloader, evaloader, optimizer, net, current_epoch, criterion, evalset_length, evalset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(epochs, trainloader, evaloader, optimizer, net, current_epoch, criterion, evalset_length, evalset):\n",
    "    plt.ion()\n",
    "    if current_epoch == -1:\n",
    "        #If not resuming a model, creating the loss file\n",
    "        lossFile = open(os.path.join(MAIN_FOLDER,\"loss\"),'wb')\n",
    "        pickle.dump({\"loss_train\":{}, \"loss_val\":{}},lossFile)\n",
    "        lossFile.close()\n",
    "    \n",
    "    start_epoch = current_epoch + 1\n",
    "    for epoch in range(start_epoch, epochs):  # loop over the dataset multiple times\n",
    "        print(\"Epoch number {}\".format(epoch))\n",
    "        #plotKeypointsOverOutputModel(0,evalset,net,IMAGES_FOLDER)#Displaying the result over the first element of the evalset\n",
    "        running_loss = 0.0\n",
    "\n",
    "        #For each epoch, we keep the loss under a dictionnary with epoch_nb as key and list of loss as value\n",
    "        lossFile = open(os.path.join(MAIN_FOLDER,\"loss\"),'rb')\n",
    "        loss_dic = pickle.load(lossFile)\n",
    "        lossFile.close()\n",
    "        lossFile = open(os.path.join(MAIN_FOLDER,\"loss\"),'wb')\n",
    "        loss_dic['loss_train'][epoch] = []\n",
    "        loss_dic['loss_val'][epoch] = []\n",
    "        pickle.dump(loss_dic,lossFile)\n",
    "        lossFile.close()\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            print(\"Batch number {}\".format(i))\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('Trainset loss[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            #Save the loss_train in disk for each batch\n",
    "            lossFile = open(os.path.join(MAIN_FOLDER,\"loss\"),'rb')  \n",
    "            loss_dic = pickle.load(lossFile)\n",
    "            lossFile.close()\n",
    "            lossFile = open(os.path.join(MAIN_FOLDER,\"loss\"),'wb')\n",
    "            loss_dic['loss_train'][epoch] += [loss.data[0]]\n",
    "            pickle.dump(loss_dic,lossFile)\n",
    "            lossFile.close()\n",
    "        \n",
    "        #Save the model\n",
    "        #net.cpu()\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': net.state_dict()\n",
    "        }\n",
    "        torch.save(state, os.path.join(MAIN_FOLDER,\"model_\"+str(epoch))) #Save the torch model after each epoch\n",
    "        \n",
    "        #net.cuda()\n",
    "        running_loss_eval = 0.0\n",
    "        print(\"Starting Eval for Epoch {}\".format(epoch))\n",
    "        for i, data in enumerate(evaloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # forward \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss_eval += loss.data[0]\n",
    "\n",
    "            #Save the loss_val in disk for each batch\n",
    "            lossFile = open(os.path.join(MAIN_FOLDER,\"loss\"),'rb')  \n",
    "            loss_dic = pickle.load(lossFile)\n",
    "            lossFile.close()\n",
    "            lossFile = open(os.path.join(MAIN_FOLDER,\"loss\"),'wb') \n",
    "            loss_dic['loss_val'][epoch] += [loss.data[0]]\n",
    "            pickle.dump(loss_dic,lossFile)\n",
    "            lossFile.close()\n",
    "\n",
    "        print(\"Evalset Loss for Epoch {0} : {1}\".format(epoch,running_loss_eval/evalset_length))\n",
    "        #loss_val[epoch] += [running_loss_eval/evalset_length] #Stock the loss on evalset for each epoch\n",
    "    \n",
    "        \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "def launch_training(resuming=False, input_type=0, *args):\n",
    "    \"\"\"Function that configurates the model from init or a last model ; and then it trains the model\"\"\"\n",
    "    epochs, trainloader, evaloader, optimizer, net, current_epoch, criterion, evalset_length, evalset = conf_training(resuming=resuming,input_type=input_type, *args)\n",
    "    training(epochs, trainloader, evaloader, optimizer, net, current_epoch, criterion, evalset_length, evalset)\n",
    "\n",
    "def launch_testing(model_epoch, input_type=0):\n",
    "    \"\"\"Function that launches a model over the test dataset\"\"\"\n",
    "    testset = MSCOCO(IMAGES_FOLDER_TEST, ANNOTATION_FILE_TEST,input_type=input_type)\n",
    "\n",
    "    #Load the training model\n",
    "    checkpoint = torch.load(os.path.join(MAIN_FOLDER, model_epoch))\n",
    "    net = Model(input_type=input_type)\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Batch sizes\n",
    "    batch_size_test = 1\n",
    "\n",
    "    #TestLoader\n",
    "    evaloader = torch.utils.data.DataLoader(testset,\n",
    "                                            batch_size=batch_size_test,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=4\n",
    "                                            )\n",
    "\n",
    "    loss_test = 0.0\n",
    "    for i, data in enumerate(evaloader):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(y, outputs)\n",
    "        loss_test += loss.data[0]\n",
    "        if i % 500 ==0:\n",
    "            print(\"Current loss over the test dataset: {0} after {1}Ã¨me iteration\".format(loss_test/(i+1),i+1))\n",
    "\n",
    "    loss_test = loss_test/len(testset)\n",
    "    print(\"Average loss over the test dataset: {}\".format(loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=21.31s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=38.47s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not visible\n",
      "not visible\n",
      "not visible\n",
      "Batch number 0\n",
      "not visible\n",
      "Batch number 1\n",
      "not visible\n",
      "Batch number 2\n",
      "Batch number 3\n",
      "not visible\n",
      "Batch number 4\n",
      "not visible\n",
      "Batch number 5\n",
      "Batch number 6\n",
      "not visible\n",
      "Batch number 7\n",
      "Batch number 8\n",
      "Batch number 9\n",
      "not visible\n",
      "Batch number 10\n",
      "Batch number 11\n",
      "Batch number 12\n",
      "not visible\n",
      "Batch number 13\n",
      "not visible\n",
      "Batch number 14\n",
      "Batch number 15\n",
      "not visible\n",
      "Batch number 16\n",
      "not visible\n",
      "Batch number 17\n",
      "Batch number 18\n",
      "Batch number 19\n",
      "not visible\n",
      "not visible\n",
      "Batch number 20\n",
      "Batch number 21\n",
      "Batch number 22\n",
      "Batch number 23\n",
      "Batch number 24\n",
      "Batch number 25\n",
      "Batch number 26\n",
      "not visible\n",
      "Batch number 27\n",
      "not visible\n",
      "Batch number 28\n",
      "not visible\n",
      "Batch number 29\n",
      "Batch number 30\n",
      "Batch number 31\n",
      "not visible\n",
      "Batch number 32\n",
      "not visible\n",
      "Batch number 33\n",
      "not visible\n",
      "Batch number 34\n",
      "not visible\n",
      "Batch number 35\n",
      "not visible\n",
      "Batch number 36\n",
      "Batch number 37\n",
      "Batch number 38\n",
      "Batch number 39\n",
      "Batch number 40\n",
      "Batch number 41\n",
      "Batch number 42\n",
      "Batch number 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/alexandresioufi/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b890a75aec99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Launch a training over a new model with inputSize = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlaunch_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-36ccb9012828>\u001b[0m in \u001b[0;36mlaunch_training\u001b[0;34m(resuming, input_type, *args)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m\"\"\"Function that configurates the model from init or a last model ; and then it trains the model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalset_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresuming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresuming\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalset_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlaunch_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-36ccb9012828>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epochs, trainloader, evaloader, optimizer, net, current_epoch, criterion, evalset_length, evalset)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-407b7c2689cc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mheatmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_to_heatmaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mheatmaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-407b7c2689cc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Launch a training over a new model with inputSize = 0\n",
    "launch_training(False,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Launch a training over a model currently trained with inputSize = 0\n",
    "#launch_training(True,0,path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Launch a trained model over the test dataset, with inputSize = 0\n",
    "#launch_testing(path_model,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexandresioufi/Documents/Projets infos/deeplearning/dl_project/cocoapi\n",
      "\u001b[34mLuaAPI\u001b[m\u001b[m      \u001b[34mPythonAPI\u001b[m\u001b[m   \u001b[34mcommon\u001b[m\u001b[m      \u001b[34mresults\u001b[m\u001b[m\r\n",
      "\u001b[34mMatlabAPI\u001b[m\u001b[m   README.txt  license.txt\r\n"
     ]
    }
   ],
   "source": [
    "%cd cocoapi\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
